import importlib

import pytest
import torch

from forge_cute_py.ops import softmax_online
from forge_cute_py.ref import softmax_online as ref_softmax_online

softmax_online_ops = importlib.import_module("forge_cute_py.ops.softmax_online")


@pytest.fixture(autouse=True)
def _reset_softmax_impl_env(monkeypatch):
    monkeypatch.delenv("FORGE_SOFTMAX_IMPL", raising=False)


def _patch_missing_kernel_module(monkeypatch):
    original_import_module = softmax_online_ops.importlib.import_module

    def missing_kernel_module(name, *args, **kwargs):
        if name == "forge_cute_py.kernels.softmax_online":
            exc = ModuleNotFoundError(f"No module named '{name}'")
            exc.name = name
            raise exc
        return original_import_module(name, *args, **kwargs)

    monkeypatch.setattr(softmax_online_ops.importlib, "import_module", missing_kernel_module)


@pytest.mark.parametrize("shape", [(4, 8), (2, 128)])
@pytest.mark.parametrize("dim", [-1, 0, 1])
@pytest.mark.parametrize(
    "dtype, atol, rtol",
    [
        (torch.bfloat16, 1e-3, 1e-3),
        (torch.float16, 1e-2, 1e-2),
        (torch.float32, 1e-4, 1e-4),
    ],
)
def test_softmax_online_correctness(shape, dim, dtype, atol, rtol):
    x = (0.1 * torch.randn(*shape, device="cuda", dtype=dtype)).requires_grad_(True)
    y = softmax_online(x, dim)
    y_ref = ref_softmax_online(x, dim=dim)
    torch.testing.assert_close(y, y_ref, atol=atol, rtol=rtol)

    assert torch.isfinite(y).all()


@pytest.mark.parametrize("shape", [(4, 8), (2, 128)])
@pytest.mark.parametrize("dim", [-1, 0, 1])
@pytest.mark.parametrize(
    "dtype, atol, rtol",
    [
        (torch.bfloat16, 1e-3, 1e-3),
        (torch.float16, 1e-2, 1e-2),
        (torch.float32, 1e-4, 1e-4),
    ],
)
def test_softmax_online_torch_compile(shape, dim, dtype, atol, rtol):
    unsupported_exc = ()
    try:
        from torch._dynamo.exc import Unsupported as DynamoUnsupported

        unsupported_exc = (DynamoUnsupported,)
    except Exception:
        unsupported_exc = ()
    try:
        compiled = torch.compile(softmax_online, fullgraph=True)
    except Exception as exc:
        pytest.skip(f"torch.compile not available for softmax_online: {exc}")
    x = torch.randn(shape, device="cuda", dtype=dtype)
    try:
        y = compiled(x, dim=dim)
    except unsupported_exc as exc:
        pytest.skip(f"torch.compile unsupported for softmax_online op: {exc}")
    y_ref = ref_softmax_online(x, dim=dim)
    torch.testing.assert_close(y, y_ref, atol=atol, rtol=rtol)


def test_softmax_online_auto_falls_back_to_ref_when_kernel_missing(monkeypatch):
    monkeypatch.setenv("FORGE_SOFTMAX_IMPL", "auto")
    _patch_missing_kernel_module(monkeypatch)

    x = torch.randn(4, 8, device="cuda", dtype=torch.float16)
    y = softmax_online(x, dim=-1)
    y_ref = ref_softmax_online(x, dim=-1)
    torch.testing.assert_close(y, y_ref, atol=1e-2, rtol=1e-2)


def test_softmax_online_kernel_mode_requires_kernel(monkeypatch):
    monkeypatch.setenv("FORGE_SOFTMAX_IMPL", "kernel")
    _patch_missing_kernel_module(monkeypatch)

    x = torch.randn(4, 8, device="cuda", dtype=torch.float16)
    with pytest.raises(NotImplementedError, match="FORGE_SOFTMAX_IMPL=kernel"):
        softmax_online(x, dim=-1)


def test_softmax_online_ref_mode_skips_kernel_probe(monkeypatch):
    import_called = {"value": False}
    original_import_module = softmax_online_ops.importlib.import_module

    def tracking_import(name, *args, **kwargs):
        if name == "forge_cute_py.kernels.softmax_online":
            import_called["value"] = True
            raise AssertionError("Kernel module should not be imported in ref mode")
        return original_import_module(name, *args, **kwargs)

    monkeypatch.setenv("FORGE_SOFTMAX_IMPL", "ref")
    monkeypatch.setattr(softmax_online_ops.importlib, "import_module", tracking_import)

    x = torch.randn(4, 8, device="cuda", dtype=torch.float16)
    y = softmax_online(x, dim=-1)
    y_ref = ref_softmax_online(x, dim=-1)

    assert import_called["value"] is False
    torch.testing.assert_close(y, y_ref, atol=1e-2, rtol=1e-2)


def test_softmax_online_rejects_invalid_impl_mode(monkeypatch):
    monkeypatch.setenv("FORGE_SOFTMAX_IMPL", "unknown")
    x = torch.randn(4, 8, device="cuda", dtype=torch.float16)
    with pytest.raises(ValueError, match="FORGE_SOFTMAX_IMPL"):
        softmax_online(x, dim=-1)


@pytest.mark.parametrize("input_dtype", [torch.float16, torch.float32])
def test_softmax_online_properties(input_dtype):
    x = torch.randn(16, 256, device="cuda", dtype=input_dtype)
    y = softmax_online(x, -1)
    sums = torch.sum(y, dim=-1)
    torch.testing.assert_close(sums, torch.ones_like(sums), atol=1e-3, rtol=1e-3)
    assert (y >= 0).all()
    assert (y <= 1).all()


def test_softmax_online_translation_invariance():
    x = torch.randn(8, 128, device="cuda", dtype=torch.float32)
    y = softmax_online(x, -1)
    y_shifted = softmax_online(x + 100.0, -1)
    torch.testing.assert_close(y, y_shifted, atol=1e-5, rtol=1e-5)


@pytest.mark.parametrize("input_dtype", [torch.float16, torch.float32])
def test_softmax_online_extreme_values(input_dtype):
    m, n = 8, 256
    x_large = torch.full((m, n), 10.0, device="cuda", dtype=input_dtype)
    out_large = softmax_online(x_large, -1)
    expected = torch.full_like(out_large, 1.0 / n)
    torch.testing.assert_close(out_large, expected, atol=1e-3, rtol=1e-3)
    x_small = torch.full((m, n), -10.0, device="cuda", dtype=input_dtype)
    out_small = softmax_online(x_small, -1)
    torch.testing.assert_close(out_small, expected, atol=1e-3, rtol=1e-3)
    x_mixed = torch.zeros((m, n), device="cuda", dtype=input_dtype)
    x_mixed[:, 0] = 10.0
    x_mixed[:, 1:] = -10.0
    out_mixed = softmax_online(x_mixed, -1)
    assert (out_mixed[:, 0] > 0.99).all()
    assert (out_mixed[:, 1:] < 0.01).all()


@pytest.mark.parametrize("shape", [(4, 8), (16, 128), (32, 256)])
@pytest.mark.parametrize("dim", [-1, 0, 1])
@pytest.mark.parametrize(
    "dtype, atol, rtol",
    [
        (torch.bfloat16, 1e-2, 1e-2),
        (torch.float16, 1e-3, 1e-3),
        (torch.float32, 1e-4, 1e-4),
    ],
)
def test_softmax_online_backward(shape, dim, dtype, atol, rtol):
    """Test backward pass against PyTorch reference."""
    # Create inputs with gradients enabled (scale by 0.1 to avoid overflow)
    x = (0.1 * torch.randn(*shape, device="cuda", dtype=dtype)).requires_grad_(True)
    x_ref = x.detach().clone().requires_grad_(True)

    # Forward pass
    out = softmax_online(x, dim=dim)
    out_ref = ref_softmax_online(x_ref, dim=dim)
    torch.testing.assert_close(out, out_ref, atol=atol, rtol=rtol)

    # Backward pass
    dy = torch.randn_like(out)
    torch.cuda.synchronize()  # Critical: prevents autograd timing issues
    (dx,) = torch.autograd.grad(out, x, grad_outputs=dy)
    (dx_ref,) = torch.autograd.grad(out_ref, x_ref, grad_outputs=dy)
    torch.testing.assert_close(dx, dx_ref, atol=atol, rtol=rtol)


@pytest.mark.parametrize("shape", [(4, 8), (16, 128)])
@pytest.mark.parametrize("dim", [-1, 1])
@pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
def test_softmax_online_backward_torch_compile(shape, dim, dtype):
    """Test backward pass works with torch.compile."""
    unsupported_exc = ()
    try:
        from torch._dynamo.exc import Unsupported as DynamoUnsupported

        unsupported_exc = (DynamoUnsupported,)
    except Exception:
        unsupported_exc = ()

    try:
        compiled = torch.compile(softmax_online, fullgraph=True)
    except Exception as exc:
        pytest.skip(f"torch.compile not available: {exc}")

    # Create inputs
    x = (0.1 * torch.randn(*shape, device="cuda", dtype=dtype)).requires_grad_(True)
    x_ref = x.detach().clone().requires_grad_(True)

    # Forward + backward (compiled)
    try:
        y = compiled(x, dim=dim)
    except unsupported_exc as exc:
        pytest.skip(f"torch.compile unsupported: {exc}")

    dy = torch.randn_like(y)
    torch.cuda.synchronize()
    (dx,) = torch.autograd.grad(y, x, grad_outputs=dy)

    # Forward + backward (reference)
    y_ref = ref_softmax_online(x_ref, dim=dim)
    torch.cuda.synchronize()
    (dx_ref,) = torch.autograd.grad(y_ref, x_ref, grad_outputs=dy)

    # Check gradients
    atol, rtol = (1e-2, 1e-2) if dtype == torch.float16 else (1e-4, 1e-4)
    torch.testing.assert_close(dx, dx_ref, atol=atol, rtol=rtol)


def test_softmax_online_gradient_properties():
    """Test mathematical properties of softmax gradients."""
    m, n = 16, 128

    # Test property: gradient of uniform upstream should have zero row/col sum
    x = torch.randn(m, n, device="cuda", dtype=torch.float32, requires_grad=True)
    y = softmax_online(x, dim=-1)

    # Uniform upstream gradient
    dy_uniform = torch.ones_like(y)
    torch.cuda.synchronize()
    (dx,) = torch.autograd.grad(y, x, grad_outputs=dy_uniform)

    # Row sums should be approximately zero (softmax Jacobian property)
    row_sums = dx.sum(dim=-1)
    torch.testing.assert_close(row_sums, torch.zeros_like(row_sums), atol=1e-6, rtol=1e-6)
